{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reducing Features Using Principal Components Analysis\n"
      ],
      "metadata": {
        "id": "D1eSVASYv09v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PCA :-Principal Component Analysis**\n",
        "\n",
        "**PCA only works with numerical data**"
      ],
      "metadata": {
        "id": "A738EjEP1SEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To Learn PCA : **\n",
        "https://towardsdatascience.com/principal-component-analysis-for-dimensionality-reduction-115a3d157bad\n",
        "\n",
        "https://machinelearningmastery.com/principal-components-analysis-for-dimensionality-reduction-\n"
      ],
      "metadata": {
        "id": "F9KF_blQv2t1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem**\n",
        "Given a set of features, you want to reduce the number of features while retaining the variance in the data.\n",
        "\n",
        "**Solution**\n",
        "Use principal component analysis with scikit's PCA:\n"
      ],
      "metadata": {
        "id": "hgMv7JhdB7Bh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fw9ZT5Qus3z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebcf112e-4313-4087-ea29-7989389e4f43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of features: 64\n",
            "Reduced number of features: 54\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import datasets\n",
        "\n",
        "# Load the data\n",
        "digits = datasets.load_digits ()\n",
        "\n",
        "# Standardize the feature matrix\n",
        "X = StandardScaler().fit_transform (digits.data)\n",
        "\n",
        "# Create a PCA that will retain 99% of the variance\n",
        "pca = PCA (n_components = 0.99,whiten=True)\n",
        "\n",
        "# Conduct PCA\n",
        "X_pca = pca.fit_transform (X)\n",
        "\n",
        "# Show results\n",
        "print ('Original number of features:', X.shape[1])\n",
        "print ('Reduced number of features:', X_pca.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA Documentation\n",
        "##Reducing Features When Data Is Linearly Inseparable\n"
      ],
      "metadata": {
        "id": "bOOMQ_vnByLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem**\n",
        "You suspect you have linearly inseparable data and want to reduce the dimensions.\n",
        "\n",
        "**Solution**\n",
        "Use an extension of principal component analysis that uses kernels to allow for non-linear dimen- sionality reduction:\n"
      ],
      "metadata": {
        "id": "pDQr78xhC6dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "from sklearn.datasets import make_circles\n",
        "\n",
        "X, _ = make_circles(n_samples=1000,random_state=1,factor=0.3,noise=0.05)\n",
        "\n",
        "kpca = KernelPCA(kernel=\"rbf\",gamma=15,n_components=1)\n",
        "X_kcpa = kpca.fit_transform(X)\n",
        "\n",
        "print('Original number of features:',X.shape[1])\n",
        "print('Reduced number of features:',X_kcpa.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL9_rnaC-NxU",
        "outputId": "a70488f3-1314-4eba-cfc3-83ac2d60307d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of features: 2\n",
            "Reduced number of features: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Kernel PCA\n",
        "Kernel PCA is an extension of PCA that allows for the separbility of non linear data by making use of kernel\n",
        "\n",
        "In our solution\n",
        "\n",
        "we used scikit-learn's make_circles to generate a simulated dataset with a target vector of two classes and two features. make_circles makes linearly inseparable data; specifically, one class is surrounded on all sides by the other class.\n"
      ],
      "metadata": {
        "id": "1VikLDFgDp96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel PCA Documentation\n",
        "**Reducing Features by Maximizing Class Separability**"
      ],
      "metadata": {
        "id": "vqdRoknGD3CK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem**\n",
        "You want to reduce the features to be used by a classifier.\n",
        "\n",
        "**Solution**\n",
        "Try linear discriminant analysis (LDA) to project the features onto component axes that maximize the separation of classes:\n"
      ],
      "metadata": {
        "id": "l_q_SFUWELdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "iris = datasets.load_iris ()\n",
        "x=iris.data\n",
        "y=iris.target\n",
        "\n",
        "lda = LinearDiscriminantAnalysis(n_components=2)\n",
        "X_lda = lda.fit_transform(x,y)\n",
        "\n",
        "print('Original number of features:',x.shape[1])\n",
        "print('Reduced number of features:',X_lda.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzNqXN5OETNE",
        "outputId": "673ac8bb-b543-40ae-99f3-35df1543b3fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of features: 4\n",
            "Reduced number of features: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use explained_variance_ratio_ to view the amount of variance explained by each com- ponent. In our solution the single component explained over 99% of the variance"
      ],
      "metadata": {
        "id": "YIH8RLB5Ftzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda.explained_variance_ratio_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0I6WPXcTFudp",
        "outputId": "4a0985af-3c49-4965-fe2b-1784db63fe66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.9912126, 0.0087874])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}
